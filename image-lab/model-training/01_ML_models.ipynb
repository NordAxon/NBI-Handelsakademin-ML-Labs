{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf': conda)"
  },
  "interpreter": {
   "hash": "00355bbddba1116272bba4883f6d673f226b023cbceb401470502a5ceca7c612"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Welcome to the second part of the ML image analysis and classification workshop!\n",
    "\n",
    "In this notebook you will implement a very basic CNN model to try to classify the images\n",
    "\n",
    "You will use Keras to implement the CNN. This is because Keras generally is easier to understand compared to PyTorch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(200,) (200, 340, 410, 3)\n(51,) (51, 340, 410, 3)\n(66,) (66, 340, 410, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def read_dataset(path: Path, labels = {\"normal\": 0, \"viral pneumonia\": 1, \"covid\": 2}) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        A function that reads a part of a dataset and saves it in numpy arrays\n",
    "    \"\"\"\n",
    "    # rglob throughs to get all file extensions\n",
    "    paths = list(path.rglob('*.jpeg')) + list(path.rglob('*.jpg')) + list(path.rglob('*.png'))\n",
    "    \n",
    "    # one list for labels, one for samples\n",
    "    set_labels = []\n",
    "    set_samples = []\n",
    "    # Iterate through each path\n",
    "    for path in paths:\n",
    "        # Get the class name and read the image\n",
    "        class_name = path.parent.name\n",
    "        image = cv2.imread(str(path))\n",
    "        # Append to each list\n",
    "        set_samples.append(image)\n",
    "        set_labels.append(labels[class_name.lower()])\n",
    "    # normalize and return\n",
    "    return np.asarray(set_labels), np.asarray(set_samples)/255\n",
    "\n",
    "train_labels, train_samples = read_dataset(Path(\"../Covid19-dataset/processed/train\"))\n",
    "train_samples, val_samples, train_labels, val_labels = train_test_split(train_samples, train_labels, test_size=0.2, random_state=42)\n",
    "test_labels, test_samples = read_dataset(Path(\"../Covid19-dataset/processed/test\"))\n",
    "\n",
    "print(train_labels.shape, train_samples.shape)\n",
    "print(val_labels.shape, val_samples.shape)\n",
    "print(test_labels.shape, test_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "y_train = to_categorical(train_labels)\n",
    "y_val = to_categorical(val_labels)\n",
    "y_test = to_categorical(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_6 (Conv2D)            (None, 338, 408, 32)      896       \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 169, 204, 32)      0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 167, 202, 32)      9248      \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 83, 101, 32)       0         \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 83, 101, 32)       0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 268256)            0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 128)               34336896  \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 3)                 387       \n=================================================================\nTotal params: 34,347,427\nTrainable params: 34,347,427\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Basic model for the NN\n",
    "model = models.Sequential()\n",
    "# Conv net\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(340, 410, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "# Classifier\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "#Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "        optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "        metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 8s 2s/step - loss: 7.1333 - acc: 0.4050 - val_loss: 3.0756 - val_acc: 0.2745\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 8s 2s/step - loss: 3.3948 - acc: 0.4400 - val_loss: 0.6566 - val_acc: 0.7059\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.9290 - acc: 0.4900 - val_loss: 0.6921 - val_acc: 0.7255\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 8s 2s/step - loss: 1.2274 - acc: 0.5450 - val_loss: 1.3186 - val_acc: 0.3137\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.7951 - acc: 0.6900 - val_loss: 1.1151 - val_acc: 0.4706\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 9s 2s/step - loss: 1.0550 - acc: 0.5100 - val_loss: 0.5840 - val_acc: 0.8039\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.5785 - acc: 0.7850 - val_loss: 0.5382 - val_acc: 0.7255\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.5840 - acc: 0.7700 - val_loss: 0.4756 - val_acc: 0.8627\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.5389 - acc: 0.7700 - val_loss: 0.5679 - val_acc: 0.7843\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.7742 - acc: 0.6850 - val_loss: 0.4732 - val_acc: 0.8235\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17b45ae89d0>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "callbacks_list = [\n",
    "    EarlyStopping(\n",
    "    monitor='acc',\n",
    "    patience=5,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "    filepath='models/my_CNN_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train model\n",
    "model.fit(train_samples, y_train, \n",
    "          epochs=10, \n",
    "          batch_size=64,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=(val_samples, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                 precision    recall  f1-score   support\n\n         normal       0.43      0.65      0.52        20\nviral pneumonia       0.59      0.50      0.54        20\n          covid       1.00      0.73      0.84        26\n\n       accuracy                           0.64        66\n      macro avg       0.67      0.63      0.63        66\n   weighted avg       0.70      0.64      0.65        66\n\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "labels = {\"normal\": 0, \"viral pneumonia\": 1, \"covid\": 2}\n",
    "\n",
    "best_model = load_model('models/my_CNN_model.h5')\n",
    "predictions = best_model.predict(test_samples)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_test_ = np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_test_, y_pred, target_names=list(labels.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}