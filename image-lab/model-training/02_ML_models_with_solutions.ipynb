{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('tf': conda)"
    },
    "interpreter": {
      "hash": "00355bbddba1116272bba4883f6d673f226b023cbceb401470502a5ceca7c612"
    },
    "colab": {
      "name": "01_ML_models.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pcemsOB5_Qa"
      },
      "source": [
        "# Welcome to the second part of the ML image analysis and classification workshop!\n",
        "\n",
        "In this notebook you will implement a very basic CNN model to try to classify the images that you analysed in the previous lab.\n",
        "\n",
        "You will use the Keras framework to implement a CNN. This is because Keras generally is easier to understand compared to PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUVLSBUkbrWR"
      },
      "source": [
        "## <ins>STARTING COMMANDS<ins>\n",
        "1. Start by changing the runtime type to GPU. Do this by selecting __runtime__ in the menu, and then __change runtime type__. Choose GPU.\n",
        "\n",
        "\n",
        "2. Then run these cells below, to bring all files into the directory and get you up to speed :-)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDCw2OBaM_EC",
        "outputId": "24861bf3-013c-4b67-de82-1301e87a88ff"
      },
      "source": [
        "# We start by running this cell to make sure that all relevant files are present in the folder structure\n",
        "!git clone https://github.com/NordAxon/NBI-Handelsakademin-ML-Labs.git\n",
        "\n",
        "# We move the Kaggle API token to where Colab wants it\n",
        "!mkdir -p ~/.kaggle/ && mv NBI-Handelsakademin-ML-Labs/kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# We download the kaggle dataset\n",
        "!kaggle datasets download -d suddirutten/covid-xray-modified\n",
        "\n",
        "# And we unzip the dataset and put it in the image-lab folder :-)\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/covid-xray-modified.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/NBI-Handelsakademin-ML-Labs/image-lab\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NBI-Handelsakademin-ML-Labs'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 77 (delta 18), reused 68 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (77/77), done.\n",
            "Downloading covid-xray-modified.zip to /content\n",
            " 70% 12.0M/17.3M [00:00<00:00, 41.9MB/s]\n",
            "100% 17.3M/17.3M [00:00<00:00, 57.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63LwWGTsNqTU"
      },
      "source": [
        "# In case you did not finish the first part of the lab, this cell will convert the raw data into the processed images :-)\n",
        "# You can add your own code here and run that instead if you wish\n",
        "# Feel free to change the hyperparameters to the CLAHE function as you see fit\n",
        "\n",
        "from collections import defaultdict\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Point to dataset path\n",
        "path_to_images = Path(\"/content/NBI-Handelsakademin-ML-Labs/image-lab/Covid19-dataset/raw\")\n",
        "# rglob through and make sure to get all file extensions\n",
        "all_paths = list(path_to_images.rglob('*.jpeg')) + list(path_to_images.rglob('*.jpg')) + list(path_to_images.rglob('*.png'))\n",
        "\n",
        "# Create empty list and empty dict\n",
        "images_list = []\n",
        "set_sizes = defaultdict(lambda: defaultdict(int))\n",
        "# Loop thorugh each path that we have\n",
        "for image_path in all_paths:\n",
        "    set_name = image_path.parent.parent.name\n",
        "    image_class = image_path.parent.name\n",
        "    set_sizes[set_name][image_class] += 1\n",
        "    image = cv2.imread(str(image_path))\n",
        "    images_list.append((image_path, image))\n",
        "\n",
        "# Saving the CLAHE transformed images under processed\n",
        "for path, img in images_list:\n",
        "    # --- Applying clahe -----\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    cl1 = clahe.apply(img[:,:,0])\n",
        "\n",
        "    # --- Saving new image under processed ----\n",
        "    new_path = path.parents[3] / \"processed\" / path.parents[1].name / path.parent.name\n",
        "    if not new_path.exists():\n",
        "      new_path.mkdir(parents=True)\n",
        "    cv2.imwrite(str(new_path / path.name), cl1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4u28C3XQlXy"
      },
      "source": [
        "## <ins>BACKGROUND</ins>\n",
        "When training a model, you will need to split your dataset into 3 subsets.\n",
        "\n",
        "These sets are called __training set__, __validation set__ and finally the __test set__.\n",
        "\n",
        "As you have noticed, the dataset is currently split only into training and test set folders. Therefore, your first task is related to splitting the dataset.\n",
        "\n",
        "When splitting datasets, normally we use the names `X` and `y` for the tensors (matrices) that we are creating. The `X` matrix will consist of the samples of a given set and the `y` matrix will consist of the labels that correspond to each sample in `X`. \n",
        "\n",
        "- It is __VERY IMPORTANT__ that the order of the labels in the `y` matrix is corresponding to the order of the samples in `X`. \n",
        "- It is also __VERY IMPORTANT__ that no samples are overlapping between the sets - i.e. every image should only be present in one set. \n",
        "\n",
        "## <ins>EXERCISE 1 - Dataset splitting</ins>\n",
        "Your first task is to read the images from the Covid19-dataset/processed/ folder into 3 separate lists. These lists should be created:\n",
        "- `X_train` = a numpy array of all training images. In this list, you will put 80% of the images found in the folder /processed/train.\n",
        "- `y_train` = a numpy array consisting of the labels for the `X_train` list. You can use the following class to number mapping: `labels = {\"normal\": 0, \"viral pneumonia\": 1, \"covid\": 2}`. Make sure that this list corresponds to the order of the images in `X_train`: if the first image in `X_train` is of class `normal`, then the first entry in the `y_train` list should be `0`.\n",
        "- `X_val` = a numpy array of all validation images. In this list, you will put 20% of the images found in the folder /processed/train.\n",
        "- `y_val` = a numpy array consisting of the labels for the `val_samples` list. Make sure that this list corresponds to the order of the images in `val_samples`: if the first image in `val_samples` is of class `covid`, then the first entry in the `val_labels` list should be `2`.\n",
        "- `X_test` = a numpy array of all images found in the folder /processed/test\n",
        "- `y_test` = a numpy array consisting of the labels of the classes. Make sure that this list corresponds to the order of the images in `test_sample`.\n",
        "\n",
        "\n",
        "#### When the lists are created, you will one-hot encode the __label__ lists. You can do this by using the keras method `to_categorical`.\n",
        "\n",
        "After this exercise, you should be able to answer the following questions:\n",
        "- How many samples are there in the training and validation sets?\n",
        "- How did the labels change from `{0,1,2}` when you one-hot encoded them?\n",
        "- How many channels are there in each image?\n",
        "\n",
        "\n",
        "### Optional hints\n",
        "- You will make your life easier if you use the sklearn method called `train_test_split` :-)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Nt8ZhbxYuNM"
      },
      "source": [
        "# Importing some libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Enter your code from here on below :-)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV8Mal035_Qc",
        "outputId": "a4f8bffb-f524-4fd2-c1e0-e3bd0f0a3d0a"
      },
      "source": [
        "# ------ SOLUTION ---------\n",
        "\n",
        "def read_dataset(path: Path, labels = {\"normal\": 0, \"viral pneumonia\": 1, \"covid\": 2}) -> np.ndarray:\n",
        "    \"\"\"\n",
        "        A function that reads a part of a dataset and saves it in numpy arrays\n",
        "    \"\"\"\n",
        "    # rglob throughs to get all file extensions\n",
        "    paths = list(path.rglob('*.jpeg')) + list(path.rglob('*.jpg')) + list(path.rglob('*.png'))\n",
        "    \n",
        "    # one list for labels, one for samples\n",
        "    set_labels = []\n",
        "    set_samples = []\n",
        "    # Iterate through each path\n",
        "    for path in paths:\n",
        "        # Get the class name and read the image\n",
        "        class_name = path.parent.name\n",
        "        image = cv2.imread(str(path))\n",
        "        # Append to each list\n",
        "        set_samples.append(image)\n",
        "        set_labels.append(labels[class_name.lower()])\n",
        "    # normalize and return\n",
        "    return np.asarray(set_labels), np.asarray(set_samples)/255\n",
        "\n",
        "y_train, X_train = read_dataset(Path(\"/content/NBI-Handelsakademin-ML-Labs/image-lab/Covid19-dataset/processed/train\"))\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "y_test, X_test = read_dataset(Path(\"/content/NBI-Handelsakademin-ML-Labs/image-lab/Covid19-dataset/processed/test\"))\n",
        "\n",
        "print(y_train.shape, X_train.shape)\n",
        "print(y_val.shape, X_val.shape)\n",
        "print(y_test.shape, X_test.shape)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train_enc = to_categorical(y_train)\n",
        "y_val_enc = to_categorical(y_val)\n",
        "y_test_enc = to_categorical(y_test)\n",
        "\n",
        "print(y_train[0:5], '\\n', y_train_enc[0:5])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200,) (200, 340, 410, 3)\n",
            "(51,) (51, 340, 410, 3)\n",
            "(66,) (66, 340, 410, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAbLL5RKZUoU"
      },
      "source": [
        "## <ins>BACKGROUND</ins>\n",
        "Now we are getting ready to actually implement and train a real ML model! We will implement a Convolutional Neural Network (CNN), and hopefully you already know the basics of how a CNN is built. Either way, here comes a recap of how to build a CNN in Keras:\n",
        "\n",
        "1. Initiate a model. This is normally done with `models.Sequential()` in Keras.\n",
        "2. Add convolutional layers and max pooling layers as you wish. Remember to specify number of filters, kernel size and activation function as hyperparameters to each convolutional layer. In the max pooling layers you can specify the pooling size.\n",
        "3. Add a flattening layer to the CNN, to squish out the tensor to a long array\n",
        "4. Normally, we add a dense layer following this, to boil down the flattened layer to a specified amount of neurons\n",
        "5. If you add more dense layers at the end of the model, remember that the final layer should have as many outputs as there are classes.\n",
        "6. Do a `model.summary()` call to see if your model compiles and the layers are working together.\n",
        "7. When you have a working model, finish it off by adding a compiling layer. For this lab, it enough to add the hyperparameters `{loss = \"categorical_crossentropy\", optimizer = optimizers.RMSprop(), metrics=[\"acc\"]}`\n",
        "\n",
        "If you want a head start, you can find a simple Keras CNN in the hidden cell below (double click \"A basic CNN\"). If you want a challenge - implement it yourself from scratch :-) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "CkCKA1ayaqmQ"
      },
      "source": [
        "#@title A basic CNN\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "# Basic model for a CNN\n",
        "model = models.Sequential()\n",
        "# Conv net\n",
        "model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "# Classifier\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='softmax'))\n",
        "model.summary()\n",
        "#Compile model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "        optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
        "        metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iivo_jruewnv"
      },
      "source": [
        "## <ins>EXERCISE 2 - Building a CNN</ins>\n",
        "\n",
        "Finally we get to the fun stuff!\n",
        "\n",
        "It is now time to implement your very own CNN. The specifications for this model is given below:\n",
        "- The number of trainable parameters should exceed 1 million\n",
        "- Use categorical crossentropy as loss function\n",
        "- Use the accuracy metric when training\n",
        "- The optimization function RMSprop should be used\n",
        "- You should train the model for at least 10 epochs\n",
        "- Use as many convolutional layers and max pooling layers as you wish\n",
        "\n",
        "\n",
        "You should be able to answer these questions when you have implemented your CNN:\n",
        "- What is the input shape of the CNN? How does this correlate to the size of the images?\n",
        "- How many trainable parameters do you have in your CNN? Do you have any non-trainable parameters? If yes, why do you think that is?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_BCkWg5bCCE"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Enter your code from here on below\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPaoJJyd5_Qe",
        "outputId": "b573bb90-e445-48ad-fb98-183f7ccdfa4d"
      },
      "source": [
        "# --------------- SOLUTION -------------------\n",
        "\n",
        "# Basic model for the NN\n",
        "model = models.Sequential()\n",
        "# Conv net\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(340, 410, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.25))\n",
        "# Classifier\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(3, activation='softmax'))\n",
        "model.summary()\n",
        "#Compile model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "        optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
        "        metrics=['acc'])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 338, 408, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 169, 204, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 167, 202, 32)      9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 83, 101, 32)       0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 83, 101, 32)       0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 268256)            0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 128)               34336896  \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 34,347,427\n",
            "Trainable params: 34,347,427\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzVkN_wbkKTI"
      },
      "source": [
        "## <ins>BACKGROUND</ins>\n",
        "\n",
        "Now we have arrived at the last step - training and evaluating the models. \n",
        "\n",
        "To make the training and saving of models as easy as possible, we have defined a `model_save_path` for you. The idea is that you will only save the best model during training, as the weights will update each epoch and the last epoch is not neccesarily the best. When we say *best* model, we mean the one that has the highest validation accuracy. \n",
        "\n",
        "We have pre-defined the `callbacks_list` for you as well, but it is up to you to figure out how to call the `callbacks_list` in the training command.\n",
        "\n",
        "## <ins>EXERCISE 3 - Training and evaluating the model</ins>\n",
        "You will now fit your model to the training data (i.e. the `X_train` and encoded `y_train` matrices).\n",
        "\n",
        "Requirements for the training:\n",
        "- You should train for at least 10 epochs\n",
        "- You should choose your batch size to a reasonable number, normally a power of 2\n",
        "- You should use your validation set during training\n",
        "\n",
        "When the model is fully trained, you will predict on the test set (that you have __not__ used in training). Print a classification report from the predictions and analyze it.\n",
        "\n",
        "You should be able to answer the following questions after the training and evaluation:\n",
        "- What is your highest validation accuracy during training?\n",
        "- What is your F1 score on the test set?\n",
        "- What is the difference between validation accuracy and F1 score on the test set? How do these normally compare?\n",
        "- Should you modify the model to better fit the test set?\n",
        "- What is the precision on the different classes? What does this mean?\n",
        "- Do you think precision or recall is most important as a metric when talking about medical applications like this?\n",
        "\n",
        "\n",
        "### Optional hints\n",
        "- Use the Keras function ´fit´ to train the model.\n",
        "- Use the `sklearn` method `classification_report´ to easily print the classification report. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Enter your code from here on below :-) Use as many cells as you wish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXXrlz51pZh0",
        "outputId": "a57fdf48-e6a8-486b-b30e-85fc91b79ab2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ------------- SOLUTION TRAINING ---------------------\n",
        "\n",
        "model_save_path = '/content/NBI-Handelsakademin-ML-Labs/image-lab/model-training/my_best_CNN_model.h5'\n",
        "\n",
        "callbacks_list = [\n",
        "    ModelCheckpoint(\n",
        "    filepath=model_save_path,\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train_enc, \n",
        "          epochs=10, \n",
        "          batch_size=64,\n",
        "          callbacks=callbacks_list,\n",
        "          validation_data=(X_val, y_val_enc))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 32s 7s/step - loss: 8.4485 - acc: 0.3550 - val_loss: 5.0396 - val_acc: 0.3333\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 29s 7s/step - loss: 3.1958 - acc: 0.3700 - val_loss: 0.8786 - val_acc: 0.4510\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 29s 7s/step - loss: 1.4025 - acc: 0.5500 - val_loss: 0.7745 - val_acc: 0.6275\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 29s 7s/step - loss: 0.9168 - acc: 0.6100 - val_loss: 0.7848 - val_acc: 0.6078\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 28s 7s/step - loss: 0.8228 - acc: 0.5950 - val_loss: 0.8712 - val_acc: 0.4118\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 28s 7s/step - loss: 0.7769 - acc: 0.6000 - val_loss: 0.8484 - val_acc: 0.6078\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 29s 7s/step - loss: 0.9153 - acc: 0.5500 - val_loss: 0.7621 - val_acc: 0.6275\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 29s 7s/step - loss: 0.6901 - acc: 0.6850 - val_loss: 0.6667 - val_acc: 0.6471\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 29s 7s/step - loss: 0.5816 - acc: 0.7550 - val_loss: 0.5259 - val_acc: 0.8235\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 29s 7s/step - loss: 0.5945 - acc: 0.7600 - val_loss: 0.6129 - val_acc: 0.7843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb2d2fc5ad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbfDLMKJ5_Qf",
        "outputId": "4709fd0e-f438-41c1-a6a2-93ae9751c748"
      },
      "source": [
        "# -------------- SOLUTION CLASSIFICATION REPORT --------------\n",
        "\n",
        "labels = {\"normal\": 0, \"viral pneumonia\": 1, \"covid\": 2}\n",
        "\n",
        "best_model = load_model(model_save_path)\n",
        "predictions = best_model.predict(X_test)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=list(labels.keys())))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         normal       0.33      0.35      0.34        20\n",
            "viral pneumonia       0.46      0.55      0.50        20\n",
            "          covid       0.95      0.77      0.85        26\n",
            "\n",
            "       accuracy                           0.58        66\n",
            "      macro avg       0.58      0.56      0.56        66\n",
            "   weighted avg       0.62      0.58      0.59        66\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Ch1II05_Qf"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}