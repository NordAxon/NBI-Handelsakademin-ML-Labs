{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "chronic-customer",
      "metadata": {
        "id": "chronic-customer"
      },
      "source": [
        "# LAB 2 - TRANSFORMERS WITH HUGGINGFACE\n",
        "\n",
        "\n",
        "In this lab you'll get to try two of the most famous NLP transformers: BERT and GPT-2\n",
        "\n",
        "To download and use the models we will use huggingface transformers, a large platform for sharing transformer models. Check it out here https://huggingface.co/\n",
        "\n",
        "https://huggingface.co/docs/transformers/index\n",
        "https://huggingface.co/docs/transformers/quicktour\n",
        "\n",
        "In this lab we will:\n",
        "- Use DistilBERT to \n",
        "        - fill out missing words in sentences, \n",
        "        - extract features from texts\n",
        "- Use DistilGPT-2 to generate a story\n",
        "\n",
        "#### TODO\n",
        "- Make sure to choose a runtime with GPU in google colab: Runtime / Change runtime type / Select GPU\n",
        "- Install huggingface transformers by running the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "occasional-trinity",
      "metadata": {
        "id": "occasional-trinity"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "extreme-figure",
      "metadata": {
        "id": "extreme-figure"
      },
      "source": [
        "# 1. BERT\n",
        "\n",
        "\n",
        "## <ins>BACKGROUND</ins>\n",
        "\n",
        "\n",
        "BERT is trained on the task of filling in masked words in sentences. We will use a distilled version of BERT made by huggingface called: DistilBERT\n",
        "\n",
        "A distilled model is simply a condensed version of a model. It performs almost as well, but is lighter and faster than the original model.\n",
        "\n",
        "We will use BERT for two things:\n",
        "1. Feature extraction\n",
        "2. Mask filling\n",
        "\n",
        "\n",
        "First we will instantiate the model and its tokenizer. In huggingface, all models are accompanied with their specific tokenizer. There are many different sorts of tokenizers, which we won't cover. One reason for this is that different models have been trained with different special characters. BERT (and DistilBERT) are specifically trained using some special tokens in the text which we'll see shortly.\n",
        "\n",
        "#### Note: The model and tokenizer we use are case-unsensitive, i.e. they don't know the difference between upper case and lower case letters. Thus to the model BERT = bert = Bert = BeRT\n",
        "\n",
        "## <ins>EXERCISE</ins>\n",
        "\n",
        "Explore the tokenizer and the model to see how they can be used as is\n",
        "\n",
        "\n",
        "#### TODO:\n",
        "- Explore the tokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "egyptian-round",
      "metadata": {
        "id": "egyptian-round"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "protective-soccer",
      "metadata": {
        "id": "protective-soccer"
      },
      "source": [
        "### Tokenizer\n",
        "\n",
        "The tokenizer is a complex object with many attributes. To get an idea of what's unique about this tokenizer you can use the cell below to see which special tokens exist, their representation in text, and how large the vocabulary of the tokenizer is.\n",
        "\n",
        "Docs:  https://huggingface.co/docs/transformers/quicktour#autotokenizer\n",
        "\n",
        "#### TODO: \n",
        "- Run the commands in the cells below to get information about the tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "numerous-google",
      "metadata": {
        "id": "numerous-google"
      },
      "outputs": [],
      "source": [
        "# The categories of special tokens used in DistilBERT\n",
        "tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "measured-beginning",
      "metadata": {
        "id": "measured-beginning"
      },
      "outputs": [],
      "source": [
        "# List the special tokens - can you guess which match which in the previous list?\n",
        "tokenizer.all_special_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "recorded-calgary",
      "metadata": {
        "id": "recorded-calgary"
      },
      "outputs": [],
      "source": [
        "# How many unique words have been used when training to tokenizer. \n",
        "# Words outside of this vocabulary will be mapped to the unk_token: '[UNK]'\n",
        "tokenizer.vocab_size"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "unusual-consciousness",
      "metadata": {
        "id": "unusual-consciousness"
      },
      "source": [
        "### Tokenize text\n",
        "\n",
        "Now we'll use the tokenizer on a sample text. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "innovative-handle",
      "metadata": {
        "id": "innovative-handle"
      },
      "outputs": [],
      "source": [
        "text = 'This is a sample sentence with fifteen words in it, one of them is antidisestablishmentarianism'\n",
        "tokens = tokenizer.encode(text)\n",
        "print('Tokens:\\n', tokens)\n",
        "print('\\nNumber of tokens: ', len(tokens))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "color-student",
      "metadata": {
        "id": "color-student"
      },
      "source": [
        "\n",
        "We also see that it's 25 tokens long even though our text only contained fifteen words. Can you figure out what happened?\n",
        "\n",
        "#### TODO:\n",
        "- Decode the input_ids using the decode() function in the tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "determined-realtor",
      "metadata": {
        "id": "determined-realtor"
      },
      "outputs": [],
      "source": [
        "# Use the decode function to see what happened\n",
        "tokenizer.decode(tokens)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "sacred-duncan",
      "metadata": {
        "id": "sacred-duncan"
      },
      "source": [
        "## Model\n",
        "\n",
        "### <ins>BACKGROUND</ins>\n",
        "\n",
        "\n",
        "Just like the tokenizer, the model variable contains a lot of information about the model. Huggingface is built on top of pytorch, and supports everything that the pytorch library enables.\n",
        "\n",
        "We might be interested to see the architecture of the model and what the inputs to the model should look like. Run the two following cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "desirable-relations",
      "metadata": {
        "id": "desirable-relations"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "immune-client",
      "metadata": {
        "id": "immune-client"
      },
      "source": [
        "## Send a sentence through model\n",
        "\n",
        "Now let's see what comes out of the model if we send a sentence through it! Run the cell below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comfortable-treasury",
      "metadata": {
        "id": "comfortable-treasury"
      },
      "outputs": [],
      "source": [
        "text = 'This sentence will go through BERT and come out the other side'\n",
        "model_input = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "output = model(**model_input)\n",
        "\n",
        "print(output.last_hidden_state)\n",
        "print('\\nShape:', output.last_hidden_state.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3qSOM-ZNkB5u",
      "metadata": {
        "id": "3qSOM-ZNkB5u"
      },
      "source": [
        "This is the output of the BERT encoder. It creates embeddings for the input text. The output is a tensor with the following dimensions: [_number_of_batches_, _number_of_tokens_, _embedding_dimensions_].\n",
        "\n",
        "So in this case we had 1 batch with a total of 14 tokens, and the embeddings are in 769 dimensions.\n",
        "\n",
        "We can then use these embeddings as inputs for other machine learning models, as they represent a very signal-rich representation of the input text.\n",
        "\n",
        "It's hard to visualize 768 dimensions, but run the cell below to visualize our input text in one dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xC50-WHjNwMA",
      "metadata": {
        "id": "xC50-WHjNwMA"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "labels = tokenizer.decode(tokenizer.encode(text)).split()\n",
        "tsne = TSNE(n_components=1, perplexity=0.9, random_state=42)\n",
        "tsne_embeddings = tsne.fit_transform(output.last_hidden_state[0,:,:].detach())\n",
        "g = sns.scatterplot(tsne_embeddings)\n",
        "g.legend_.remove()\n",
        "ticks = g.set_xticks(range(len(text.split())+2), labels=labels, rotation=45)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "boxed-superintendent",
      "metadata": {
        "id": "boxed-superintendent"
      },
      "source": [
        "# 2. Pipelines\n",
        "\n",
        "### <ins>BACKGROUND</ins>\n",
        "\n",
        "\n",
        "The output we got in the last step doesn't tell us much. We get a vector for every token in the input, but how can we use it concretely?\n",
        "\n",
        "To make it easier to use the models we'll make use of huggingfaces pipelines. Pipelines are basically wrappers for models and tokenizers that automate getting an output for a specific task. There are many pipelines you can use, see a full list here: https://huggingface.co/docs/transformers/quicktour#pipeline\n",
        "\n",
        "We will use the \"fill-mask\" and \"feature-extraction\" pipelines.\n",
        "\n",
        "Loading and using a pipeline is super easy. We only have to specify the kind of pipeline and the model we want to use and huggingface takes care of the rest!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "removable-concrete",
      "metadata": {
        "id": "removable-concrete"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n",
        "extractor = pipeline(\"feature-extraction\", model='distilbert-base-uncased')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "periodic-poison",
      "metadata": {
        "id": "periodic-poison"
      },
      "source": [
        "## Fill mask\n",
        "\n",
        "We know that BERT was originally trained to predict masked tokens in sentences. It should thus be pretty good at predicting missing words in sentences. \n",
        "\n",
        "\n",
        "\n",
        "### <ins>EXERCISE</ins>\n",
        "\n",
        "The pipeline only supports one masked token at a time. It will spit out a list of what it thinks are the most likely words to fill in and their scores.\n",
        "\n",
        "\n",
        "#### TODO:\n",
        "- Play around with the unmasker by changing the masked_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "liberal-metropolitan",
      "metadata": {
        "id": "liberal-metropolitan"
      },
      "outputs": [],
      "source": [
        "# Fill mask \n",
        "# masked_sentence = \"Hello there my friend. [MASK] are you doing today?\"\n",
        "masked_sentence = \"Hello there my [MASK]. what are you doing today?\"\n",
        "unmasker(masked_sentence)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "later-evanescence",
      "metadata": {
        "id": "later-evanescence"
      },
      "source": [
        "## Choosing the most likely candidate\n",
        "\n",
        "### <ins>EXERCISE</ins>\n",
        "\n",
        "Out of the candidates you saw, you probably thought some were better than others. The model also scored how likely it thought the different options were. A natural way to choose is to simply pick the most probable option.\n",
        "\n",
        "Using the unmasker pipeline's outputs, implement a function that you can use to fill one word in a sentence.\n",
        "\n",
        "#### TODO:\n",
        "- Complete the function fill_mask() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "executive-paint",
      "metadata": {
        "id": "executive-paint"
      },
      "outputs": [],
      "source": [
        "def fill_mask(masked_sentence):\n",
        "    \"Returns the masked sentence filled out with the most likely candidate\"\n",
        "    candidates = unmasker(masked_sentence)\n",
        "    most_likely_candidate = # Fill in with your code\n",
        "    return most_likely_candidate['sequence']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abandoned-comfort",
      "metadata": {
        "id": "abandoned-comfort"
      },
      "outputs": [],
      "source": [
        "fill_mask('Hello where can I find the [MASK]')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "attractive-prague",
      "metadata": {
        "id": "attractive-prague"
      },
      "source": [
        "# GPT-2\n",
        "\n",
        "\n",
        "## <ins>BACKGROUND</ins>\n",
        "\n",
        "GPT - Generative pre-training is a series of models released by OpenAI (GPT-3 was released in 2020). In 2019, GPT-2 got a lot of media attention in the heated debates about its potential dangers- and for writing stories about unicorns. \n",
        "\n",
        "Of course, now with ChatGPT, these debates are even hotter!\n",
        "\n",
        "Just like BERT, the GPT models are trained in an unsupervised fashion, but instead of the MLM task they are trained to do causal language modelling (CLM) - predicting the next word given the context. This arguably makes GPT-2 more fun to play around with than BERT.\n",
        "\n",
        "\n",
        "\n",
        "### Unicorn story\n",
        "\n",
        "The original unicorn story generated by the GPT-2 model trained by OpenAI.\n",
        "\n",
        "#### Written prompt\n",
        "\n",
        "> In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
        "\n",
        "#### Generated by GPT-2\n",
        "\n",
        "> The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.\n",
        "\n",
        "> Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.\n",
        "\n",
        "> Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.\n",
        "\n",
        "> Pérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez.\n",
        "\n",
        "> Pérez and his friends were astonished to see the unicorn herd. These creatures could be seen from the air without having to move too much to see them – they were so close they could touch their horns.\n",
        "\n",
        "> While examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English. Pérez stated, “We can see, for example, that they have a common ‘language,’ something like a dialect or dialectic.”\n",
        "\n",
        "> Dr. Pérez believes that the unicorns may have originated in Argentina, where the animals were believed to be descendants of a lost race of people who lived there before the arrival of humans in those parts of South America.\n",
        "\n",
        "> While their origins are still unclear, some believe that perhaps the creatures were created when a human and a unicorn met each other in a time before human civilization. According to Pérez, “In South America, such incidents seem to be quite common.”\n",
        "\n",
        "> However, Pérez also pointed out that it is likely that the only way of knowing for sure if unicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to communicate in English quite well, which I believe is a sign of evolution, or at least a change in social organization,” said the scientist.\n",
        "\n",
        "\n",
        "\n",
        "Just like we used DistilBERT, we will here be using DistilGPT-2 from huggingface, which is a distilled version of GPT-2 small.\n",
        "\n",
        "\n",
        "\n",
        "## <ins>EXERCISE</ins>\n",
        "\n",
        "In this exercise we'll use distill-gpt2 to generate text.\n",
        "\n",
        "#### TODO:\n",
        "- Use Distill-GPT2 to generate text\n",
        "- Find another model and use it to generate text\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "judicial-winter",
      "metadata": {
        "id": "judicial-winter"
      },
      "source": [
        "### Generate text with Distil-GPT2\n",
        "\n",
        "Use the cell below to download and use distilgpt2. \n",
        "\n",
        "When you execute the cell you'll get a prompt for the model input that it will generate text from. You can of course change the length and number of outputs you want it to generate! Why not try giving it the same prompt that the unicorn text was generated from?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kA1zR6-0UFaM",
      "metadata": {
        "id": "kA1zR6-0UFaM"
      },
      "outputs": [],
      "source": [
        "!pip install xformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "tlYx8BWkUhqu",
      "metadata": {
        "id": "tlYx8BWkUhqu"
      },
      "outputs": [],
      "source": [
        "prompt = \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pQmPNSXOTQIU",
      "metadata": {
        "id": "pQmPNSXOTQIU"
      },
      "outputs": [],
      "source": [
        "generator = pipeline(task=\"text-generation\", model=\"distilgpt2\")\n",
        "\n",
        "generator(prompt, max_new_tokens=50, num_return_sequences=4, return_full_text=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2Hyk5uQUiuTQ",
      "metadata": {
        "id": "2Hyk5uQUiuTQ"
      },
      "source": [
        "### Other models\n",
        "\n",
        "In the same way we did above, we can load other models. There are thousands available on Huggingface, including ones trained on Swedish data. \n",
        "\n",
        "Find another model for text generation on https://huggingface.co/ and create a generator for it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DFEYkRGFi-nV",
      "metadata": {
        "id": "DFEYkRGFi-nV"
      },
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "lZiyJty8iAkO",
      "metadata": {
        "id": "lZiyJty8iAkO"
      },
      "source": [
        "## Now what?\n",
        "\n",
        "Feel like you're done playing around with generating text? Explore the Huggingface documentation, and perhaps find a tutorial on how you can fine tune a pretrained model on your own text! https://huggingface.co/docs/transformers/main/en/quicktour"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "huggingface",
      "language": "python",
      "name": "huggingface"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
