{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqjP4gTNRdCI"
      },
      "source": [
        "# LAB 1 - NLP\n",
        "Start by running the cell below to install the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ueqZADZRdCO"
      },
      "outputs": [],
      "source": [
        "!pip install numpy gensim pandas matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKXEq4iIRdCP"
      },
      "source": [
        "# 1. Tokenizers\n",
        "\n",
        "### <ins>BACKGROUND</ins> \n",
        "\n",
        "Tokenizers split a text into a sequence of tokens. There are different ways of doing this but an intuitive first step towards a good tokenizer is one that simply splits a text into it's constituent words. You will first construct such a tokenizer and then build a better one.\n",
        "\n",
        "### <ins>EXERCISE 1</ins> \n",
        "\n",
        "#### TODO: \n",
        "- Implement a simple tokenizer that can tokenize by splitting a text on whitespaces. \n",
        "- Test it on a sample text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfJyr2H5RdCQ"
      },
      "outputs": [],
      "source": [
        "def simpletokenizer(text):\n",
        "    \"Split the text on whitespaces\"\n",
        "    tokens = # Write your code here\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q83hiRmhRdCR"
      },
      "source": [
        "### Test your tokenizer on this text\n",
        "\n",
        "Now we would like to test our function `simpletokenizer` on some text. Our sample text consists of the first three paragraphs from the book [Bannlyst by Selma Lagerlöf](http://runeberg.org/bannlyst/0003.html).\n",
        "\n",
        "Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrDojFLJRdCR"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"På Grimön i den västra skärgården bodde för några år sedan en man och en hustru, som voro varandra mycket olika. \n",
        "Mannen, som var omkring femton år äldre än hustrun, hade aldrig varit annat än ful, trög och senfärdig och hade inte blivit bättre på gamla dar, hustrun åter hade alltid varit nätt och lätt, och hennes lilla vackra ansikte hade bibehållit sig så väl, att hon såg närapå lika bra ut vid femtio som vid tjugu. \n",
        "Dessa två makar sutto en vacker söndagskväll på en stor stenhäll, som stack upp ur marken alldeles utanför deras hus, och samspråkade i god ro. Mannen, som tyckte om att höra sin egen röst och lade sina ord väl, utbredde sig för hustrun över en artikel, som han just nyss hade läst i en tidning.\n",
        "Hustrun hörde på honom med inte alltför spänd uppmärksamhet. Ack, den Joel, den Joel, tänkte hon, att han kan få ut så mycken lärdom ur ett sådant där tidningsblad! Han har verkligen ett märkvärdigt gott huvud. Det är bara synd, att han inte är i stånd att göra något bruk av det för sin och min räkning, utan bara för andras.\"\"\"\n",
        "\n",
        "\n",
        "simpletokenizer(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfGWyQdaRdCS"
      },
      "source": [
        "### What do you see?\n",
        "You should see a python list of words. If you look closely, you'll see there are some items in the list that aren't just one word. There are also special characters in some that we probably want to remove.\n",
        "\n",
        "#### Questions\n",
        "\n",
        "- What special characters are left, that we would like to remove?\n",
        "- What does `\\n` mean?\n",
        "\n",
        "Tip: Remember that you can search the web if you don't know the answer!\n",
        "\n",
        "### Next steps\n",
        "\n",
        "We now have two options to improve our tokenizer function:\n",
        "- **A**. create a function `better_tokenizer` that handles the special characters and other special cases that you find\n",
        "- **B**. use regex to create a function `regextokenizer`\n",
        "\n",
        "**Option A** is a good exercise for practicing your skills with strings, lists and loops.\n",
        "\n",
        "**Option B** is good for practicing regex. Even if you find it intimidating, at least try to read one of the linked tutorials to get a feeling for what it can do. There is always more things to learn, this is just one of many!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <ins>EXERCISE 2A - Strings, lists, and loops! </ins>\n",
        "\n",
        "We can clean up the word list by looping through it and remove punctuation, new line characters and other things we have found that should be removed.\n",
        "\n",
        "#### TODO: \n",
        "- Implement a tokenizer that tokenizes and removes all special characters\n",
        "    - How do you handle new line characters?\n",
        "    - How do you handle multiple words in one string?\n",
        "    - How do you handle punctuation?\n",
        "    - Tip: go through [this list of string methods](https://www.w3schools.com/python/python_ref_string.asp) you can apply to all strings to see if any of them look useful\n",
        "    - Tip: check out Python's [list methods](https://www.w3schools.com/python/python_ref_list.asp) to get clues on how to specify where in a list you want to put new elements\n",
        "- Run the test (you can find it in a cell further below) to see that it works properly"
      ],
      "metadata": {
        "id": "eb2r6uBUf8xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def better_tokenizer(text):\n",
        "    \"\"\" This function should return a list of only words in lowercase, without any special characters. \"\"\"\n",
        "\n",
        "    string_list = text.split(\" \")\n",
        "\n",
        "    # write your code here\n",
        "    # what do you want to do for each string in the string_list?\n",
        "\n",
        "    tokens = []\n",
        "\n",
        "    for string in string_list:\n",
        "        pass\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "v_3uC0veh6gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "better_tokenizer(text)"
      ],
      "metadata": {
        "id": "waC3f3w3ZdFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### <ins>EXERCISE 2B - Regex </ins>\n",
        "\n",
        "#### What is regex?\n",
        "\n",
        "Regex is a very powerful tool to manipulate text in many different ways. \n",
        "\n",
        "More technically, regex is a pattern matcher. The Python module `re` contains many functions for regex, and using the `re` module looks like\n",
        "\n",
        "    re.function(regex, text)\n",
        "\n",
        "where regex is the regular expression and text the string you want to search for the pattern in. When using regexes in python, it's important to tell python that the regex is a 'raw string' by putting an 'r' before the string like this regex for finding sequences of whitespaces in text:\n",
        "\n",
        "    r'\\s+'\n",
        "\n",
        "**However!** regex can be confusing. Please do go through one of the tutorials, either the one on RealPython or Regexone. \n",
        "\n",
        "Regex is valuable to be aware of, so even if you do not use it here, now you know it exists and is something you can come back to in the future!\n",
        "\n",
        "#### TODO: \n",
        "- Implement a tokenizer that tokenizes and removes all special characters with the python regex module 're'\n",
        "    - Doc at: https://docs.python.org/3/howto/regex.html\n",
        "    - Python Docs can be hard to follow, here is a more gentle introduction: https://realpython.com/regex-python/\n",
        "    - Interactive course on regex: https://regexone.com/\n",
        "    - Try regular expressions at https://regex101.com/  (make sure to select python in the list to the left)\n",
        "    - Tip: There are different regexes that will solve the problem\n",
        "    - Tip: a word can be described as the letters between two word boundaries\n",
        "    - Tip: chatGPT is not great at regex, it often suggests very complicated solutions that do not give you the result you want\n",
        "- Run the test (you can find it in a cell further below) to see that it works properly"
      ],
      "metadata": {
        "id": "QnmKZj7Shmiq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70M_gzg7RdCT"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def regextokenizer(text):\n",
        "    \"\"\"This function should return a list of only words in lowercase, without any special characters. \n",
        "    Fill in your regex pattern in the findall method\"\"\"\n",
        "    \n",
        "    regex_pattern = r'\\w+' # Write your regex pattern here, e.g. r'[a-z]+'\n",
        "    \n",
        "    return re.findall(regex_pattern, text) # make sure to only return words in lowercase"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regextokenizer(text)"
      ],
      "metadata": {
        "id": "vaJZ2NDfAKXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBH42H4LRdCU"
      },
      "source": [
        "### Test your improved tokenizer!\n",
        "When you are done with your regextokenizer, run the cell below to check that it works properly\n",
        "\n",
        "**NOTE** This test can be used for either the regex tokenizer or the \"better\" tokenizer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOAPpj6vRdCU"
      },
      "outputs": [],
      "source": [
        "def test_tokenizer(tokens):\n",
        "    assert isinstance(tokens, list), 'did not receive a list of tokens'\n",
        "    assert len(tokens) != 0, 'received empty list'\n",
        "    assert sum(len(x) for x in tokens) != len(tokens), 'Looks like all your tokens are single characters'\n",
        "    passed = 1\n",
        "    if any([',' in token for token in tokens]):\n",
        "        print('There are commas in your tokens. Make sure they are removed')\n",
        "        passed = 0\n",
        "    if any(['!' in token for token in tokens]):\n",
        "        print('There are excalmation points in your tokens. Make sure they are removed')\n",
        "        passed = 0\n",
        "    if any(['?' in token for token in tokens]):\n",
        "        print('There are question marks in your tokens. Make sure they are removed')\n",
        "        passed = 0\n",
        "    if any(['.' in token for token in tokens]):\n",
        "        print('There are periods in your tokens. Make sure they are removed')\n",
        "        passed = 0\n",
        "    if any(['\\n' in token for token in tokens]):\n",
        "        print(r'There are newlines(\\n) in your tokens. Make sure they are removed')\n",
        "        passed = 0\n",
        "    if any([[char.isupper() for char in token] for token in tokens]):\n",
        "        print(r'There are upper case letters in your tokens. Make sure they are set to lower')\n",
        "        passed = 0\n",
        "    else:\n",
        "        print('Your tokens have no special characters left!')\n",
        "    return\n",
        "\n",
        "\n",
        "# change below to the tokenizer function you want to test\n",
        "tokens = regextokenizer(text)\n",
        "# tokens = better_tokenizer(text)\n",
        "\n",
        "test_tokenizer(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOFg4-JeRdCV"
      },
      "source": [
        "# 1. TF-IDF \n",
        "\n",
        "\n",
        "### <ins> BACKGROUND </ins>\n",
        "\n",
        "The TF-IDF or \"Term frequency inverse document frequency\" is a measure of how important a certain word is to a certain text. It can for example be used in a search engine to find documents/webpages that are relevant to your query.\n",
        "\n",
        "You can read more about TF-IDF\n",
        "- https://monkeylearn.com/blog/what-is-tf-idf/\n",
        "- https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
        "\n",
        "Quick explanation:\n",
        "\n",
        "Term Frequency (TF) means how common a certain term (word) is in a single document. A document could be anything from a single page of text, to one whole book as in our case below.\n",
        "\n",
        "Inverse Document Frequency (IDF) means how common a certain term (word) is across many documents (or many books).\n",
        "\n",
        "In mathematical terms, we can write this in Python pseudo code as:\n",
        "\n",
        "- TF[word] = nbr_occurances[word] / nbr_words_in_text\n",
        "- IDF[word] = log10 (nbr_documents / nbr_documents_that_contain_word)\n",
        "- TF-IDF[word]: TF[word] * IDF[word] \n",
        "\n",
        "Note we use the logarithm for IDF, which in practice means we that if a word is very common and can be found in most or even all our documents, then IDF will be close to 0. If the word is uncommon/rare, then IDF will be closer to 1.\n",
        "\n",
        "#### TODO:\n",
        "\n",
        "- Read texts by selma lagerlöf by running the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNvYWyz2RdCW"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/NordAxon/NBI-Handelsakademin-ML-Labs\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Starting to read books...\")\n",
        "\n",
        "selma_dir = Path('/content/NBI-Handelsakademin-ML-Labs/nlp-lab/data/selma_lagerlof')\n",
        "files = [file for file in selma_dir.iterdir() if file.suffix == '.txt']\n",
        "texts = {}\n",
        "for file in files:\n",
        "    with open(file,'r', encoding=\"utf8\") as f:\n",
        "        text = f.read()\n",
        "        texts[file.stem] = text\n",
        "\n",
        "print('Read books:')\n",
        "for title in texts.keys():\n",
        "    print(' ', title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0xVUH1eRdCW"
      },
      "source": [
        "### <ins> EXERCISE </ins>\n",
        "\n",
        "\n",
        "You now have a dictionary where the book title maps to the text. Now you have to calculate the Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
        "\n",
        "\n",
        "\n",
        "#### TODO:\n",
        "- implement the function term_frequency()\n",
        "    - You need to standardize and tokenize the text.\n",
        "    - The function should return a dict like object, with the words of the text as keys, and their frequency in the text as values\n",
        "    - Tip: You can read more about how to use Counter here: https://realpython.com/python-counter/#getting-started-with-pythons-counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-7v-YnFRdCW"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def term_frequency(text):\n",
        "    \"\"\"Tokenizes text and returns a dictionary like object tf_dict[word].\n",
        "    \n",
        "    Example:\n",
        "        input:\n",
        "            text = han hade det fint i sitt rum\n",
        "        output:\n",
        "            tf_dict = {\n",
        "                'han': 0.14285714285714285,\n",
        "                'hade': 0.14285714285714285,\n",
        "                'det': 0.14285714285714285,\n",
        "                'fint': 0.14285714285714285,\n",
        "                'i': 0.14285714285714285,\n",
        "                'sitt': 0.14285714285714285,\n",
        "                'rum': 0.14285714285714285}\n",
        "    \"\"\"\n",
        "    # Write your code below. Hint: Use your tokenizer from before \n",
        "\n",
        "    return tf_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test your term_frequency function on the sample text\n",
        "text = \"han hade det fint i sitt rum\"\n",
        "term_frequency(text)"
      ],
      "metadata": {
        "id": "JH8zaJJdZ5DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LRX971FRdCX"
      },
      "source": [
        "#### Run the cell below to create a master tf for all the books"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX96DHuORdCX"
      },
      "outputs": [],
      "source": [
        "def term_frequency_all_texts(texts):\n",
        "    \"Returns a nested dictionary where master_tf['title'] returns the term frequency dictionary for the book 'title'\"\n",
        "    master_tf = {}\n",
        "    for title, text in texts.items():\n",
        "        master_tf[title] = term_frequency(text)\n",
        "    \n",
        "    return master_tf\n",
        "\n",
        "master_tf = term_frequency_all_texts(texts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the structure of our master_tf is what we expect\n",
        "# before you run this cell, ask yourself what you expect to see\n",
        "# should it be a list? a dictionary? a dictionary with dictionaries?\n",
        "master_tf"
      ],
      "metadata": {
        "id": "LooAHiKm3XjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy4vBnAiRdCX"
      },
      "source": [
        "### Try out your tf \n",
        "\n",
        "Access the term frequency of a word in a book with \n",
        "\n",
        "    master_tf['title']['word']\n",
        "\n",
        "If it works correctly, the term frequency of 'nils' in 'nils' should be  0.0003979"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAByf4vPRdCY"
      },
      "outputs": [],
      "source": [
        "master_tf['nils']['nils']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SOOEm_ERdCY"
      },
      "source": [
        "### IDF\n",
        "\n",
        "To create the IDF (Inverse Document Frequency) you need to know in how many documents a certain word exists. The IDF is then calculated as \n",
        "\n",
        "    IDF(word) = log10 (nbr_documents / nbr_documents_that_contain_word)\n",
        "\n",
        "**Note:** nbr_documents is short for number of documents, or \"antal dokument\" in Swedish.\n",
        "\n",
        "#### TODO:\n",
        "- Complete the function `make_inverse_document_frequency()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0fgtH17RdCY"
      },
      "outputs": [],
      "source": [
        "from numpy import log10\n",
        "\n",
        "def make_inverse_document_frequency(master_tf):\n",
        "    word_in_nbr_docs = {} # vocabulary of words\n",
        "    number_of_documents = len(master_tf)\n",
        "    \n",
        "    # master_tf contains all the words from all the individual documents\n",
        "    for book_title, term_frequency in master_tf.items():\n",
        "        \n",
        "        for word in term_frequency.keys():\n",
        "        # We add the word to the vocabulary\n",
        "            try:\n",
        "                word_in_nbr_docs[word] += 1\n",
        "            except KeyError:\n",
        "                word_in_nbr_docs[word] = 1\n",
        "    \n",
        "    # word -> idf\n",
        "    word_to_idf = {}\n",
        "    \n",
        "    for word, counts in word_in_nbr_docs.items():\n",
        "        # Calculate the idf \n",
        "        word_to_idf[word] = None # Insert your code here\n",
        "        \n",
        "    return word_to_idf   \n",
        "\n",
        "idf = make_inverse_document_frequency(master_tf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if structure makes sense\n",
        "idf"
      ],
      "metadata": {
        "id": "ZeVcieOV3b1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9EbMdcwRdCY"
      },
      "source": [
        "### Try out your IDF\n",
        "\n",
        "If it works properly, idf['nils'] should be ~0.2553"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sG04vxfMRdCZ"
      },
      "outputs": [],
      "source": [
        "idf[\"nils\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn5Iy-qvRdCZ"
      },
      "source": [
        "### Finally it's time to compute the TF-IDF from our TF and IDF values\n",
        "\n",
        "We want a nested dictionary so we can look up the TF-IDF for a certain word as `tf_idf[text][word]`\n",
        "\n",
        "#### TODO:\n",
        "- implement make_tf_idf()\n",
        "\n",
        "Hints:\n",
        "- Recall that `TF-IDF[word] = TF[word] * IDF[word]`\n",
        "- It should have the same structure as the master_tf object. \n",
        "- You can iterate through the object and update the values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE0e1G2zRdCZ"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def make_tf_idf(master_tf, idf):\n",
        "    \n",
        "    # Use if you want. You need to go through the tfidf and update all the values\n",
        "    # tf_idf = deepcopy(master_tf)\n",
        "    \n",
        "    # Fill in your code here  \n",
        "    \n",
        "    return tf_idf\n",
        "\n",
        "tf_idf = make_tf_idf(master_tf, idf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if structure makes sense\n",
        "tf_idf"
      ],
      "metadata": {
        "id": "PauD4fX02sjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5GrIXDHRdCZ"
      },
      "source": [
        "## Testing your TF-IDF\n",
        "\n",
        "You now have a TF-IDF table that you can use to get values from different texts. Use it to compare the values of different words between the different texts.\n",
        "\n",
        "\n",
        "#### TODO:\n",
        "- Look at some tfidf values. What conclusions can you draw from your results?\n",
        "    Here's a list of words you can try:\n",
        "    + nils\n",
        "    + och\n",
        "    + värmland\n",
        "    + troll\n",
        "    + pojke\n",
        "- Use the function k_most_specific_words to get a list of the words with the largest tfidf values for the different texts\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what book titles do we have?\n",
        "tf_idf.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dBF857u3p5X",
        "outputId": "d5fbe4a3-1e2f-4e72-cc52-1626b5997444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['nils', 'bannlyst', 'marbacka', 'gosta', 'osynliga', 'jerusalem', 'kejsaren', 'troll', 'herrgard'])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# look at specific words\n",
        "tf_idf[title][word]"
      ],
      "metadata": {
        "id": "fTeu0UGS3yRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iusLuLfRdCa"
      },
      "source": [
        "#### TODO:\n",
        "\n",
        "- Find the words with the highest tfidf values in the texts. Do they make sense?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkoX5dPpRdCa"
      },
      "outputs": [],
      "source": [
        "def k_most_specific_words(title, k):\n",
        "    c = Counter(tf_idf[title])\n",
        "    return c.most_common(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6jyI0jERdCa"
      },
      "outputs": [],
      "source": [
        "k_most_specific_words('gosta', 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_most_specific_words(\"nils\", 10)"
      ],
      "metadata": {
        "id": "RxKE3m_F4euJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Something to think about\n",
        "\n",
        "How would you use the concepts of TF, IDF, and TF-IDF to create, for example, a search engine?"
      ],
      "metadata": {
        "id": "PZJzW3gK4v8h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ_Qo8jTRdCb"
      },
      "source": [
        "# 3. Word embeddings\n",
        "\n",
        "To become familiar with word embeddings we'll use the gensim library. We'll download pretrained word2vec embeddings trained on google news.\n",
        "\n",
        "The gensim library provides functions for playing around with the word embeddings such as comparing how similar two words are, listing the most similar words and more\n",
        "\n",
        "**TIP**: check the lecture slides for a refresher on what embeddings are!\n",
        "\n",
        "### <ins> EXERCISE </ins>\n",
        "\n",
        "##### TODO:\n",
        "- Play around with the different functions and get a feel for how you can query a word\n",
        "- When you run the below cell it will take 5-10 minutes to download, so here's your chance to take a short break!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqinyZo7RdCb",
        "outputId": "002695e4-2fdd-44b7-9d1c-2d98c5cf9bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=================================================-] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCClMqHiRdCb"
      },
      "source": [
        "### How similar are two words\n",
        "\n",
        "Which companies have the highest pairwise similarity according to the word embeddings: Google, Facebook, Apple and Microsoft? Compare them and see!\n",
        "\n",
        "Tip: use itertools to construct all of the combinations so you don't have to do it manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhOf3LdMRdCb"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "companies = ['google', 'facebook', 'microsoft', 'apple']\n",
        "combinations = None\n",
        "\n",
        "word1 = 'google'\n",
        "word2 = 'facebook'\n",
        "\n",
        "print(f'Similarity between {word1} and {word2} is {wv.similarity(word1,word2)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WvFpdgVRdCb"
      },
      "source": [
        "### List the most similar words\n",
        "\n",
        "We can query the most similar words to a certain word. Do you think they are reasonable?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-dj1H8VRdCb"
      },
      "outputs": [],
      "source": [
        "word = 'google'\n",
        "wv.similar_by_word(word, topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"apple\"\n",
        "wv.similar_by_word(word, topn=10)"
      ],
      "metadata": {
        "id": "gsOH888PFYj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxpl6RF5RdCc"
      },
      "source": [
        "### Similar to X like Y to Z\n",
        "\n",
        "We can query that a word should be positively similar to certain words, but not similar to other words. \n",
        "\n",
        "We can use this to query an answer to the following question:\n",
        "\"A king is to a man what a woman is to a ...\"\n",
        "\n",
        "Can you find other examples where a word should be similar to certain words but not to other to find the answer you're looking for?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h99OqipMRdCc"
      },
      "outputs": [],
      "source": [
        "wv.most_similar(positive=[\"man\", \"queen\"], negative=[\"king\"], topn=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnK7hOXRRdCc"
      },
      "source": [
        "### You can also try to use basic linear math operators to construct a vector and check what word it is closest to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmxyqg3qRdCk"
      },
      "outputs": [],
      "source": [
        "new_word = wv['tiger'] - wv['cat'] + wv['dog']\n",
        "wv.most_similar(positive=[new_word], topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU588VWpRdCk"
      },
      "source": [
        "### Similar to X but not Y\n",
        "\n",
        "Some words are ambiguous - Apple can be both a tech company and a fruit. What if you want to find similar words to just the fruit? Or just the company? With the 'negative' argument you can specify a word you don't want it to be similar to.\n",
        "\n",
        "Can you find other examples of ambiguous words?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDvwTnAIRdCk"
      },
      "outputs": [],
      "source": [
        "wv.most_similar(positive=[\"apple\"], negative=[\"fruit\", 'company'], topn=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-QaF-ljRdCl"
      },
      "source": [
        "### Find the odd one out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ3L3btzRdCl"
      },
      "outputs": [],
      "source": [
        "words = \"apple pear fruit bear \" # Separate the words by a space\n",
        "print(wv.doesnt_match(words.split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azSjB0WSRdCl"
      },
      "source": [
        "## Plot embeddings\n",
        "\n",
        "The embeddings we've used have the dimension 300, i.e. every word is represented by 300 values. In a mathematical sense, they exist in an \"abstract room\" with 300 dimensions. It's in this room that we compare how similar words are by computing distances and angles between words.\n",
        "\n",
        "Obviously we can't visualise something in 300 dimensions. But there are mathematical tricks for reducing the dimensions that we can use to then plot the points in 2D(or 3D). Here we'll use an algorithm called TSNE to plot words similar to a target word.\n",
        "\n",
        "### <ins> EXERCISE </ins>\n",
        "\n",
        "#### TODO:\n",
        "\n",
        "- Choose a target word and plot the embeddings that are close to it\n",
        "\n",
        "\n",
        "#### Trivia\n",
        "\n",
        "In the call to the `TSNE` function below, notice it says `init='pca'`? \n",
        "\n",
        "PCA, or Principal Component Analysis, is a form of *unsupervised learning*. Remember what that is from the first lecture?\n",
        "\n",
        "That's right! Unsupervised learning automagically finds patterns in data. In this case, it can find the most important _two_ dimensions (`n_components` in the TSNE call) that contain the most information out of the full 300 dimensions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4iZaY9qRdCl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "\n",
        "target_word = \"mario\" # This will be the center of your embedding space\n",
        "\n",
        "\n",
        "selected_words = [w[0] for w in wv.most_similar(positive=[target_word], topn=50)] + [target_word]\n",
        "embeddings = [wv[w] for w in selected_words] + wv[target_word]\n",
        "\n",
        "mapped_embeddings = TSNE(n_components=2, metric='cosine', init='pca').fit_transform(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nerxKeIRRdCl"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "x = mapped_embeddings[:,0]\n",
        "y = mapped_embeddings[:,1]\n",
        "plt.scatter(x, y)\n",
        "\n",
        "for i, word in enumerate(selected_words):\n",
        "    if word == target_word:\n",
        "        plt.annotate(word, (x[i], y[i]), color=\"red\")\n",
        "    else:\n",
        "        plt.annotate(word, (x[i], y[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3jTJyvURdCm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}