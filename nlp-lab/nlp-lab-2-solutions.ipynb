{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chronic-customer",
   "metadata": {},
   "source": [
    "# LAB 2 - TRANSFORMERS WITH HUGGINGFACE\n",
    "\n",
    "\n",
    "In this lab you'll get to try two of the most famous NLP transformers: BERT and GPT-2\n",
    "\n",
    "To download and use the models we will use huggingface transformers, a large platform for sharing transformer models. Check it out here https://huggingface.co/\n",
    "\n",
    "In this lab we will:\n",
    "- Use DistilBERT to \n",
    "        - fill out missing words in sentences, \n",
    "        - extract features from texts\n",
    "- Fine tune DistilGPT-2 on a text and then use it to generate a story\n",
    "\n",
    "Install huggingface transformers by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "occasional-trinity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (4.7.0.dev0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages\\sacremoses-0.0.43-py3.8.egg (from transformers) (0.0.43)\n",
      "Requirement already satisfied: requests in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from transformers) (4.56.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from transformers) (0.0.8)\n",
      "Requirement already satisfied: packaging in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from requests->transformers) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from sacremoses->transformers) (1.0.0)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.8.1rc2\n",
      "    Uninstalling tokenizers-0.8.1rc2:\n",
      "      Successfully uninstalled tokenizers-0.8.1rc2\n",
      "Successfully installed tokenizers-0.10.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-figure",
   "metadata": {},
   "source": [
    "# 1. BERT\n",
    "\n",
    "\n",
    "## <ins>BACKGROUND</ins>\n",
    "\n",
    "\n",
    "BERT is trained on the task of filling in masked words in sentences. We will use a distilled version of BERT made by huggingface called: DistilBERT\n",
    "\n",
    "A distilled model is simply a condensed version of a model. It performs almost as well, but is lighter and faster than the original model.\n",
    "\n",
    "We will use BERT for two things:\n",
    "1. Feature extraction\n",
    "2. Mask filling\n",
    "\n",
    "\n",
    "First we will instantiate the model and its tokenizer. In huggingface, all models are accompanied with their specific tokenizer. There are many different sorts of tokenizers, which we won't cover. One reason for this is that different models have been trained with different special characters. BERT (and DistilBERT) are specifically trained using some special tokens in the text which we'll see shortly.\n",
    "\n",
    "#### Note: The model and tokenizer we use are case-unsensitive, i.e. they don't know the difference between upper case and lower case letters. Thus to the model BERT = bert = Bert = BeRT\n",
    "\n",
    "## <ins>EXERCISE</ins>\n",
    "\n",
    "Explore the tokenizer and the model to see how they can be used as is\n",
    "\n",
    "\n",
    "#### TODO:\n",
    "- Explore the tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "egyptian-round",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-soccer",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "The tokenizer is a complex object with many attributes. To get an idea of what's unique about this tokenizer you can use the cell below to see which special tokens exist, their representation in text, and how large the vocabulary of the tokenizer is.\n",
    "\n",
    "Docs: https://huggingface.co/transformers/main_classes/tokenizer.html\n",
    "\n",
    "#### TODO: \n",
    "- Run the commands in the cells below to get information about the tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "numerous-google",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos_token',\n",
       " 'eos_token',\n",
       " 'unk_token',\n",
       " 'sep_token',\n",
       " 'pad_token',\n",
       " 'cls_token',\n",
       " 'mask_token',\n",
       " 'additional_special_tokens']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The categories of special tokens used in DistilBERT\n",
    "tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "measured-beginning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the special tokens - can you guess which match which in the previous list?\n",
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "recorded-calgary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique words have been used when training to tokenizer. \n",
    "# Words outside of this vocabulary will be mapped to the unk_token: '[UNK]'\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-consciousness",
   "metadata": {},
   "source": [
    "### Tokenize text\n",
    "\n",
    "Now we'll use the tokenizer on a sample text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "innovative-handle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      " [101, 2023, 2003, 1037, 7099, 6251, 2007, 2702, 2616, 1999, 2009, 102]\n",
      "\n",
      "Number of tokens:  12\n"
     ]
    }
   ],
   "source": [
    "text = 'This is a sample sentence with ten words in it'\n",
    "tokens = tokenizer.encode(text)\n",
    "print('Tokens:\\n', tokens)\n",
    "print('\\nNumber of tokens: ', len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-student",
   "metadata": {},
   "source": [
    "\n",
    "We also see that it's 12 tokens long even though our text only contained ten words. Can you figure out what happened?\n",
    "\n",
    "#### TODO:\n",
    "- Decode the input_ids using the decode() function in the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "determined-realtor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] this is a sample sentence with ten words in it [SEP]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the decode function to see what happened\n",
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-duncan",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### <ins>BACKGROUND</ins>\n",
    "\n",
    "\n",
    "Just like the tokenizer, the model variable contains a lot of information about the model. Huggingface is built on top of pytorch, and supports everything that the pytorch library enables.\n",
    "\n",
    "We might be interested to see the architecture of the model and what the inputs to the model should look like. Run the two following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "desirable-relations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "motivated-midwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[7, 6, 0, 0, 1],\n",
       "         [1, 2, 3, 0, 0],\n",
       "         [0, 0, 0, 4, 5]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dummy_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-client",
   "metadata": {},
   "source": [
    "## Send a sentence through model\n",
    "\n",
    "Now let's see what comes out of the model if we send a sentence through it! Run the cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "comfortable-treasury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1551, -0.0323,  0.0877,  ...,  0.0033,  0.2715,  0.2747],\n",
      "         [-0.1971, -0.0893,  0.1830,  ...,  0.0153,  0.3042,  0.1955],\n",
      "         [ 0.3238, -0.0349,  0.0759,  ...,  0.0196, -0.1462, -0.2677],\n",
      "         ...,\n",
      "         [-0.7359, -0.3485,  0.2496,  ..., -0.2540,  0.5690, -0.4983],\n",
      "         [-0.3434, -0.2095, -0.2392,  ...,  0.2645,  0.1149,  0.1997],\n",
      "         [ 0.9707,  0.4362, -0.1645,  ...,  0.0281, -0.4067, -0.4955]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "\n",
      "Shape: torch.Size([1, 14, 768])\n"
     ]
    }
   ],
   "source": [
    "text = 'This sentence will go through BERT and come out the other side'\n",
    "model_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "output = model(**model_input)\n",
    "\n",
    "print(output.last_hidden_state)\n",
    "print('\\nShape:', output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-superintendent",
   "metadata": {},
   "source": [
    "# 2. Pipelines\n",
    "\n",
    "### <ins>BACKGROUND</ins>\n",
    "\n",
    "\n",
    "The output we got in the last step doesn't tell us much. We get a vector for every token in the input, but how can we use it concretely?\n",
    "\n",
    "To make it easier to use the models we'll make use of huggingfaces pipelines. Pipelines are basically wrappers for models and tokenizers that automate getting an output for a specific task. There are many pipelines you can use, see a full list here: https://huggingface.co/transformers/main_classes/pipelines.html\n",
    "\n",
    "We will use the \"fill-mask\" and \"feature-extraction\" pipelines.\n",
    "\n",
    "Loading and using a pipeline is super easy. We only have to specify the kind of pipeline and the model we want to use and huggingface takes care of the rest!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "removable-concrete",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AlexanderHagelborn\\Miniconda3\\envs\\testenv\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "C:\\Users\\AlexanderHagelborn\\Miniconda3\\envs\\testenv\\lib\\site-packages\\torchaudio\\backend\\utils.py:89: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n",
    "extractor = pipeline(\"feature-extraction\", model='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-poison",
   "metadata": {},
   "source": [
    "## Fill mask\n",
    "\n",
    "We know that BERT was originally trained to predict masked tokens in sentences. It should thus be pretty good at predicting missing words in sentences. \n",
    "\n",
    "\n",
    "\n",
    "### <ins>EXERCISE</ins>\n",
    "\n",
    "The pipeline only supports one masked token at a time. It will spit out a list of what it thinks are the most likely words to fill in and their scores. Can you create a function that uses the pipeline to fill in a sentence with several masks with the most likely words?\n",
    "\n",
    "\n",
    "#### TODO:\n",
    "- Play around with the unmasker by changing the masked_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "liberal-metropolitan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'hello there my friend. what are you doing today?',\n",
       "  'score': 0.8218488693237305,\n",
       "  'token': 2054,\n",
       "  'token_str': 'what'},\n",
       " {'sequence': 'hello there my friend. how are you doing today?',\n",
       "  'score': 0.1665491908788681,\n",
       "  'token': 2129,\n",
       "  'token_str': 'how'},\n",
       " {'sequence': 'hello there my friend. where are you doing today?',\n",
       "  'score': 0.006061742547899485,\n",
       "  'token': 2073,\n",
       "  'token_str': 'where'},\n",
       " {'sequence': 'hello there my friend. why are you doing today?',\n",
       "  'score': 0.0012343761045485735,\n",
       "  'token': 2339,\n",
       "  'token_str': 'why'},\n",
       " {'sequence': 'hello there my friend. whatever are you doing today?',\n",
       "  'score': 0.000964021252002567,\n",
       "  'token': 3649,\n",
       "  'token_str': 'whatever'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill mask \n",
    "masked_sentence = \"Hello there my friend. [MASK] are you doing today?\"\n",
    "unmasker(masked_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-evanescence",
   "metadata": {},
   "source": [
    "## Choosing the most likely candidate\n",
    "\n",
    "### <ins>EXERCISE</ins>\n",
    "\n",
    "Out of the candidates you saw you probably thought some were better than others. The model also scored how likely it thought the different A natural way to choose is to simply pick the most probably. Using the unmasker pipeline's outputs, implement a function that you can use to fill one word in a sentence.\n",
    "\n",
    "#### TODO:\n",
    "- Complete the function fill_mask() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "executive-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mask(masked_sentence):\n",
    "    \"Returns the masked sentence filled out with the most likely candidate\"\n",
    "    candidates = unmasker(masked_sentence)\n",
    "    most_likely_candidate = max(candidates, key= lambda x: x['score'])\n",
    "    return most_likely_candidate['sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abandoned-comfort",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert is a sort of robot trained by google'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask('BERT is a sort of [MASK] trained by google')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-timing",
   "metadata": {},
   "source": [
    "## Filling several masks\n",
    "\n",
    "You should now implement a function that can fill several masked words so we can get around the limitations of the fill-mask pipeline. Here's a list of approaches you can try:\n",
    "\n",
    "\n",
    "#### Option 1:\n",
    "1. Replace all but one [MASK] tokens in the sentence with another token\n",
    "    - Hint: Go back and look at the special tokens DistilBERT uses\n",
    "2. Fill the [MASK] token\n",
    "3. Uncover another [MASK] token and fill it\n",
    "4. Repeat step 3 until all have been filled\n",
    "\n",
    "\n",
    "#### Option 2:\n",
    "1. Split the sentence into as many parts as there are masked words\n",
    "2. Send each piece into the function fill_mask()\n",
    "3. Stitch results together \n",
    "\n",
    "\n",
    "#### Option 3:\n",
    "If you want a more challenging solution, you can try to build many different candidates and calculate their probabilities conditional probabilities. \n",
    "\n",
    "E.g. for a the sentence: \"Hi my [MASK] is BERT and I am a [MASK]\"\n",
    "1. Get the most probable fill for each mask. Use the token for an unknown word for the other mask\n",
    "    - \"Hi my [MASK] is BERT and I am a [UNK]\" -> model -> word1=name + probability10\n",
    "    - \"Hi my [UNK] is BERT and I am a [MASK]\" -> model -> word2=model + probability20\n",
    "    \n",
    "2. Fill in each subsentence with the word BERT thought was most likely and send it back through\n",
    "    - \"Hi my name is BERT and I am a [MASK]\" -> model -> word11=human + probability11\n",
    "    - \"Hi my [MASK] is BERT and I am a model\" -> model -> word22=name + probability22\n",
    "    \n",
    "3. Calculate the probabilities of the results as a conditional probability of independent events\n",
    "    - P(\"Hi my name is BERT and I am a human\") = probability10 * probability11\n",
    "    - P(\"Hi my name is BERT and I am a model\") = probability20 * probability21\n",
    "4. Pick the most likely candidate\n",
    "\n",
    "\n",
    "#### TODO:\n",
    "- Write a function that uses the unmasker to fill in several masks\n",
    "    + Hint: to replace tokens you can use string.replace\n",
    "    + Hint: to split the sentence you can use a regex and the re.split() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "earned-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_several_masks(masked_text):\n",
    "    \"Fills several masked tokens by replacing all but one [MASK] with [mask] at a time\"\n",
    "    # Write your code here\n",
    "    modified_masked_text = masked_text.replace(\"[MASK]\", \"[mask]\").replace(\"[mask]\", \"[MASK]\", 1)\n",
    "    print(modified_masked_text)\n",
    "    sentence = fill_mask(modified_masked_text)\n",
    "    print(sentence)\n",
    "    while 'mask' in sentence:\n",
    "        sentence = sentence.replace(\"[ mask ]\", \"[MASK]\", 1)\n",
    "        print(sentence)\n",
    "        sentence = fill_mask(sentence)\n",
    "        print(sentence)\n",
    "\n",
    "    return sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-paragraph",
   "metadata": {},
   "source": [
    "### Try your function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "informed-landing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi my [MASK] is BERT and I am a [mask] used for [mask].\n",
      "hi my hat is bert and i am a [ mask ] used for [ mask ].\n",
      "hi my hat is bert and i am a [MASK] used for [ mask ].\n",
      "hi my hat is bert and i am a hat used for [ mask ].\n",
      "hi my hat is bert and i am a hat used for [MASK].\n",
      "hi my hat is bert and i am a hat used for hats.\n",
      "hi my hat is bert and i am a hat used for hats.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hi my [MASK] is BERT and I am a [MASK] used for [MASK].\"\n",
    "\n",
    "print(fill_several_masks(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-prague",
   "metadata": {},
   "source": [
    "# GPT-2\n",
    "\n",
    "\n",
    "## <ins>BACKGROUND</ins>\n",
    "\n",
    "GPT - Generative pre-training is a series of models released by OpenAI (GPT-3 was released in 2020). In 2019, GPT-2 got a lot of media attention in the heated debates about its potential dangers- and for writing stories about unicorns. \n",
    "\n",
    "Just like BERT, the GPT models are trained in an unsupervised fashion, but instead of the MLM task they are trained to do causal language modelling (CLM) - predicting the next word given the context. This arguably makes GPT-2 more fun to play around with than BERT.\n",
    "\n",
    "\n",
    "\n",
    "### Unicorn story\n",
    "\n",
    "The original unicorn story generated by the GPT-2 model trained by OpenAI.\n",
    "\n",
    "#### Written prompt\n",
    "\n",
    "> In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
    "\n",
    "#### Generated by GPT-2\n",
    "\n",
    "> The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.\n",
    "\n",
    "> Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.\n",
    "\n",
    "> Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.\n",
    "\n",
    "> Pérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez.\n",
    "\n",
    "> Pérez and his friends were astonished to see the unicorn herd. These creatures could be seen from the air without having to move too much to see them – they were so close they could touch their horns.\n",
    "\n",
    "> While examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English. Pérez stated, “We can see, for example, that they have a common ‘language,’ something like a dialect or dialectic.”\n",
    "\n",
    "> Dr. Pérez believes that the unicorns may have originated in Argentina, where the animals were believed to be descendants of a lost race of people who lived there before the arrival of humans in those parts of South America.\n",
    "\n",
    "> While their origins are still unclear, some believe that perhaps the creatures were created when a human and a unicorn met each other in a time before human civilization. According to Pérez, “In South America, such incidents seem to be quite common.”\n",
    "\n",
    "> However, Pérez also pointed out that it is likely that the only way of knowing for sure if unicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to communicate in English quite well, which I believe is a sign of evolution, or at least a change in social organization,” said the scientist.\n",
    "\n",
    "\n",
    "\n",
    "Just like we used DistilBERT, we will here be using DistilGPT-2 from huggingface, which is a distilled version of GPT-2 small.\n",
    "\n",
    "\n",
    "\n",
    "## <ins>EXERCISE</ins>\n",
    "\n",
    "In this exercise we'll use distill-gpt2 to generate text.\n",
    "\n",
    "We will fine tune distill-gpt2 with Alice in Wonderland to see what happens\n",
    "\n",
    "#### TODO:\n",
    "- Use Distill-GPT2 to generate text\n",
    "- Fine tune the model on alice in wonderland\n",
    "- Generate text with your fine-tuned model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-printing",
   "metadata": {},
   "source": [
    "## Alice in wonderland from project gutenberg\n",
    "\n",
    "We'll start by downloading alice in wonderland distributed by project gutenberg to use for fine tuning and take a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "agricultural-maple",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-06-14 21:25:18--  https://raw.githubusercontent.com/NordAxon/NBI-Handelsakademin-ML-Labs/main/nlp-lab/data/alice.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "WARNING: cannot verify raw.githubusercontent.com's certificate, issued by ‘CN=DigiCert SHA2 High Assurance Server CA,OU=www.digicert.com,O=DigiCert Inc,C=US’:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 150507 (147K) [text/plain]\n",
      "Saving to: ‘alice.txt’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 34% 1.37M 0s\n",
      "    50K .......... .......... .......... .......... .......... 68% 2.44M 0s\n",
      "   100K .......... .......... .......... .......... ......    100% 2.49M=0.07s\n",
      "\n",
      "2021-06-14 21:25:18 (1.94 MB/s) - ‘alice.txt’ saved [150507/150507]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CHAPTER I.\n",
      "Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into\n",
      "the book her sister was reading, but it had no pictures or\n",
      "conversations in it, “and what is the use of a book,” thought Alice\n",
      "“without pictures or conversations?”\n",
      "\n",
      "So she was considering in her own mind (as well as she could, for the\n",
      "hot day made her feel very sleepy and stupid), whether the pleasure of\n",
      "making a daisy-chain would be worth the trouble of getting up and\n",
      "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
      "close by her.\n",
      "\n",
      "There was nothing so _very_ remarkable in that; nor did Alice think it\n",
      "so _very_ much out of the way to hear the Rabbit say to itself, “Oh\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/NordAxon/NBI-Handelsakademin-ML-Labs/main/nlp-lab/data/alice.txt --no-check-certificate -O alice.txt\n",
    "!head -20 alice.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-spray",
   "metadata": {},
   "source": [
    "## Install huggingface transformers from source\n",
    "\n",
    "We'll use a script from huggingface in order to fine-tune and use our model so we have to install it from source. We'll also go back to a previous version of the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "rough-divorce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "intensive-giant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AlexanderHagelborn\\code\\NBI-Handelsakademin-ML-Labs\\transformers\n"
     ]
    }
   ],
   "source": [
    "cd transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "adaptive-virus",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: switching to 'v3.3.0'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 0613f0522 Release: v3.3.0\n"
     ]
    }
   ],
   "source": [
    "!git checkout v3.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "patent-intelligence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AlexanderHagelborn\\code\\NBI-Handelsakademin-ML-Labs\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "scientific-walnut",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'C:\\\\Users\\\\AlexanderHagelborn\\\\Miniconda3\\\\envs\\\\testenv\\\\Lib\\\\site-packages\\\\~okenizers\\\\tokenizers.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install -q ./transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-winter",
   "metadata": {},
   "source": [
    "### Generate text with Distil-GPT2\n",
    "\n",
    "Use the cell below to download and use distilgpt2. \n",
    "\n",
    "When you execute the cell you'll get a prompt for the model input that it will generate text from. You can of course change the length and number of outputs you want it to generate! Why not try giving it the same prompt that the unicorn text was generated from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "civilian-insulin",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file '/content/transformers/examples/text-generation/run_generation.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python /content/transformers/examples/text-generation/run_generation.py \\\n",
    "--model_type=gpt2 \\\n",
    "--model_name_or_path='distilgpt2' \\\n",
    "--num_return_sequences 3 \\\n",
    "--length 150 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-riverside",
   "metadata": {},
   "source": [
    "### Fine tune Distill-GPT2 on Alice in Wonderland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "vocal-transformation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file '/content/transformers/examples/language-modeling/run_language_modeling.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python /content/transformers/examples/language-modeling/run_language_modeling.py \\\n",
    "--model_type=gpt2 \\\n",
    "--model_name_or_path=distilgpt2 \\\n",
    "--do_train \\\n",
    "--train_data_file=/content/alice.txt \\\n",
    "--num_train_epochs 20 \\\n",
    "--output_dir model_output \\\n",
    "--overwrite_output_dir \\\n",
    "--save_steps 20000 \\\n",
    "--per_gpu_train_batch_size 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-polls",
   "metadata": {},
   "source": [
    "### Generate text using your model!\n",
    "\n",
    "Now use your fine-tuned model to generate text! You can go back and use the base model and compare the ouputs for the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/transformers/examples/text-generation/run_generation.py \\\n",
    "--model_type=gpt2 \\\n",
    "--model_name_or_path=/content/model_output \\\n",
    "--num_return_sequences 3 \\\n",
    "--length 150 \\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
